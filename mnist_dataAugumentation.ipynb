{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazakura2024/university-pytorch-DataAugmentation-fashionMNIST/blob/main/mnist_dataAugumentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "gIhY_DLPGxbS",
        "outputId": "5623457e-b4a5-4bd5-9e5e-8340f957079a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-288381547.py, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-288381547.py\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    transforms.RandomAffine(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "source": [
        "# Python標準のライブラリ。存在しないキーにアクセスした際に自動で初期値を作成する辞書(dict)機能を提供\n",
        "from collections import defaultdict\n",
        "\n",
        "# グラフ描画ライブラリ。学習の進捗や結果を可視化するために使う\n",
        "# as pltと書くことで、今後はpltという短い名前でこのライブラリの機能（例: plt.plot()）を呼び出せるように\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 数値計算ライブラリ。多次元配列(行列)を効率的に扱うために使う\n",
        "import numpy as np\n",
        "\n",
        "# PyTorchの本体。Tensorの定義や基本的な計算機能を提供\n",
        "import torch\n",
        "\n",
        "# ニューラルネットワークの層(Linear, Conv2dなど)や活性化関数、損失関数などが含まれるモジュール\n",
        "# nnの中には、全結合層 (nn.Linear) や畳み込み層 (nn.Conv2d)、活性化関数 (nn.ReLU) など、ネットワークの「層」を定義するためのクラスが多数含まれています。\n",
        "import torch.nn as nn\n",
        "\n",
        "# 画像認識用のデータセット(Fashion-MNISTなど)を簡単に利用するためのモジュール\n",
        "# torchvisionは、画像認識の分野でPyTorchを助けるためのライブラリです。\n",
        "# その中のdatasetsモジュールは、Fashion-MNISTやCIFAR10といった、\n",
        "#  機械学習の練習や研究でよく使われるデータセットを簡単にダウンロードして利用する機能を提供します。\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# 画像データに対する前処理(リサイズ、Tensorへの変換、Data Augmentationなど)を行う機能のモジュール\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# ★ 変更点: Precision, Recall, F1値を計算するために scikit-learn をインポート\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\n",
        "#Fashion-MNISTを読み込む\n",
        "\n",
        "# 名前を簡単にしておく\n",
        "# 画像データ（PILイメージやNumPy配列）を、PyTorchが計算で使えるTensor（テンソル）形式に変換します。\n",
        "data_transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ここに課題を作成する！！！\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(10),\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomAffine(\n",
        "        # degrees=10,       # rotation_range=10 と同じ\n",
        "    scale=(0.9, 1.1)  # zoom_range=0.1 (90%~110%) と同じ\n",
        "    )\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 実験の条件を記載\n",
        "augmenttationType = \"scale\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 学習データを読み込む DataLoader を作成する。\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root = \".\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = train_transform\n",
        "    )\n",
        "# root: データセットを保存するディレクトリのパスを指定します。\n",
        "# データを保存する場所として、カレントディレクトリ（今いる場所）を示す \".\" を指定\n",
        "\n",
        "# train: True を指定した場合は学習データ、False を指定した場合はテストデータをダウンロードします。\n",
        "\n",
        "# transform: 画像に対して行う前処理をdata_transform = transforms.ToTensor()指定します。今回は、transforms.ToTensor のみを指定しました。 この Transformer は、画素値を範囲が [0, 255] の uint8 型から範囲が [0, 1] の float32 型にし、Pytorch で扱う Tensor に変換を行います\n",
        "# 最低限、画像をTensorに変換する必要があるので transform=transforms.ToTensor() を指定\n",
        "\n",
        "# download: True を指定した場合は、データセットがローカルにない場合は、ネットからダウンロードします。\n",
        "# データがなければダウンロードしてほしいので download=True\n",
        "\n",
        "# これでtrain_data_loaderは、訓練ループで使うと「シャッフルされた64枚ずつの画像とラベルのバッチ」を次々と提供してくれる、便利な「供給機」になります。\n",
        "# Dataset（データのリスト）から、モデルが学習しやすいようにデータをまとめて取り出すための機能です。\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    # データを*4枚ずつの「束」（バッチ）**にして取り出すよう設\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# テストデータを読み込む DataLoader を作成する。\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root = \"datasets\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_dataset)\n",
        "\n",
        "'''\n",
        "FashionMNIST クラスの以下の属性で、クラス一覧や各画像、ラベルを取得できます。\n",
        "\n",
        "FashionMNIST.classes (list): クラス名一覧\n",
        "FashionMNIST.class_to_idx (dict): クラス名、値がクラス ID の辞書\n",
        "FashionMNIST.data (Tensor): 画像一覧\n",
        "FashionMNIST.targets (Tensor): ラベル一覧\n",
        "\n",
        "'''\n",
        "\n",
        "# 各クラスのラベルを持つサンプルを1つずつ取得する。\n",
        "class_ids, sample_indices = np.unique(train_dataset.targets, return_index=True);\n",
        "# train_dataset.targets: 学習データセット内の全画像のラベル（0: T-shirt, 1: Trouser, ... 9: Ankle boot）が格納されているリスト\n",
        "\n",
        "# np.unique(...): NumPyの関数で、重複する値を取り除いて、ユニークな値だけを返します。\n",
        "# この場合、[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] という配列が class_ids に入ります。\n",
        "\n",
        "# return_index=True: このオプションを付けると、ユニークな値が**初めて出現した位置（インデックス）**も一緒に返してくれます。\n",
        "# 例えば、「ラベル0が最初に出てきたのはデータセットの何番目か」「ラベル1が最初に出てきたのは何番目か」…という情報が sample_indices に入ります。\n",
        "# これにより、各クラスの代表サンプルを1つずつ効率的に見つけることができます。\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "# これからグラフや画像を描画するための「キャンバス」や「台紙」のようなものを作成します。figsize=(10, 4) は、キャンバスのサイズを横10インチ、縦4インチに設定しています。\n",
        "\n",
        "fig.suptitle(\"Example of every class in the Fashion-MNIST dataset だよ\",fontsize=\"x-large\")\n",
        "# 作成したキャンバス全体に大きなタイトルを設定します。\n",
        "# fontsize=\"x-large\"で文字サイズを少し大きめにしています\n",
        "\n",
        "for i in class_ids:\n",
        "  img = train_dataset.data[sample_indices[i]]\n",
        "  class_name = train_dataset.classes[i]\n",
        "\n",
        "# キャンバスを2行5列のグリッドに分割し、(i+1)番目の領域を今回の描画エリア(ax)として設定する。\n",
        "  ax = fig.add_subplot(2, 5, i + 1)\n",
        "\n",
        "# 描画エリアに、クラスIDとクラス名をタイトルとして設定する。(例: \"0: T-shirt/top\")\n",
        "  ax.set_title(f\"{i}: {class_name}\")\n",
        "\n",
        "# 画像の枠線や目盛りを非表示にする。\n",
        "  ax.set_axis_off()\n",
        "\n",
        "# 取得した画像(img)をグレースケール(cmap=\"gray\")で描画エリアに表示する。\n",
        "  ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "# これまで設定した内容をすべて画面に表示する。\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- ここからが追加するプレビューコード ---\n",
        "\n",
        "print(\"--- Data Augmentation プレビュー ---\")\n",
        "\n",
        "# Augmentation（データ拡張）の変換処理だけを定義\n",
        "# 注意: ToTensor() や Normalize() は、ここでは含めない\n",
        "#      (画像として表示しにくくなるため)\n",
        "augmentation_transform = transforms.Compose([\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    # transforms.RandomRotation(10)\n",
        "])\n",
        "\n",
        "# 元となる画像データを1つ取得\n",
        "# train_dataset.data[0] は (28, 28) のTensor\n",
        "# 変換処理(transform)はPIL Imageを期待するため、ToPILImage()で変換\n",
        "original_image_tensor = train_dataset.data[0]\n",
        "original_pil_image = transforms.ToPILImage()(original_image_tensor)\n",
        "\n",
        "# プレビューの数\n",
        "num_previews = 5\n",
        "\n",
        "# 描画用のキャンバスを作成 (元画像 + プレビュー5枚 = 合計6枚)\n",
        "fig = plt.figure(figsize=(12, 3))\n",
        "fig.suptitle(f\"Data Augmentation ({augmenttationType}) のプレビュー\", fontsize=14)\n",
        "# fig.suptitle(\"Data Augmentationなしのプレビュー\", fontsize=14)\n",
        "\n",
        "# 1. 元画像をプロット\n",
        "ax = fig.add_subplot(1, num_previews + 1, 1)\n",
        "ax.set_title(\"Original\")\n",
        "ax.imshow(original_pil_image, cmap=\"gray\")\n",
        "ax.set_axis_off()\n",
        "\n",
        "# 2. Data Augmentationを適用した画像をプロット\n",
        "for i in range(num_previews):\n",
        "    # 同じ元画像に、毎回ランダムな変換を適用\n",
        "    transformed_image = augmentation_transform(original_pil_image)\n",
        "\n",
        "    ax = fig.add_subplot(1, num_previews + 1, i + 2)\n",
        "    ax.set_title(f\"Preview {i + 1}\")\n",
        "    ax.imshow(transformed_image, cmap=\"gray\")\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# グラフを表示\n",
        "plt.show()\n",
        "\n",
        "# --- ここまでが追加するプレビューコード ---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "i_O58tf9HzH4",
        "outputId": "9bc56c57-3d2f-422f-cd52-4622c6fdd0a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2594631374.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# PyTorchの本体。Tensorの定義や基本的な計算機能を提供\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ニューラルネットワークの層(Linear, Conv2dなど)や活性化関数、損失関数などが含まれるモジュール\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   6705\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6706\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6707\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_refs/special/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m @_make_elementwise_unary_reference(\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mELEMENTWISE_TYPE_PROMOTION_KIND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINT_TO_FLOAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(prim)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0maten_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_aten_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maten_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mregister_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mdecomposition_decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map_\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m     \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# consume and exhaust the iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m_add_op_to_registry\u001b[0;34m(registry, op, fn)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpOverloadPacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop_overload\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverloads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0muse_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"default\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;31m# TODO: disallow access to overloads registered by JIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             op_dk_tags = torch._C._get_operation_overload(\n\u001b[0m\u001b[1;32m   1204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qualified_op_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Python標準のライブラリ。存在しないキーにアクセスした際に自動で初期値を作成する辞書(dict)機能を提供\n",
        "from collections import defaultdict\n",
        "\n",
        "# グラフ描画ライブラリ。学習の進捗や結果を可視化するために使う\n",
        "# as pltと書くことで、今後はpltという短い名前でこのライブラリの機能（例: plt.plot()）を呼び出せるように\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 数値計算ライブラリ。多次元配列(行列)を効率的に扱うために使う\n",
        "import numpy as np\n",
        "\n",
        "# PyTorchの本体。Tensorの定義や基本的な計算機能を提供\n",
        "import torch\n",
        "\n",
        "# ニューラルネットワークの層(Linear, Conv2dなど)や活性化関数、損失関数などが含まれるモジュール\n",
        "# nnの中には、全結合層 (nn.Linear) や畳み込み層 (nn.Conv2d)、活性化関数 (nn.ReLU) など、ネットワークの「層」を定義するためのクラスが多数含まれています。\n",
        "import torch.nn as nn\n",
        "\n",
        "# 画像認識用のデータセット(Fashion-MNISTなど)を簡単に利用するためのモジュール\n",
        "# torchvisionは、画像認識の分野でPyTorchを助けるためのライブラリです。\n",
        "# その中のdatasetsモジュールは、Fashion-MNISTやCIFAR10といった、\n",
        "#  機械学習の練習や研究でよく使われるデータセットを簡単にダウンロードして利用する機能を提供します。\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# 画像データに対する前処理(リサイズ、Tensorへの変換、Data Augmentationなど)を行う機能のモジュール\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# ★ 変更点: Precision, Recall, F1値を計算するために scikit-learn をインポート\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "\n",
        "\n",
        "#Fashion-MNISTを読み込む\n",
        "\n",
        "# 名前を簡単にしておく\n",
        "# 画像データ（PILイメージやNumPy配列）を、PyTorchが計算で使えるTensor（テンソル）形式に変換します。\n",
        "data_transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ここに課題を作成する！！！\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(10),\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomAffine(\n",
        "            degrees=10,       # rotation_range=10 と同じ\n",
        "            scale=(0.9, 1.1)  # zoom_range=0.1 (90%~110%) と同じ\n",
        "        ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 実験の条件を記載\n",
        "augmenttationType = \"Web\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 学習データを読み込む DataLoader を作成する。\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root = \".\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = train_transform\n",
        "    )\n",
        "# root: データセットを保存するディレクトリのパスを指定します。\n",
        "# データを保存する場所として、カレントディレクトリ（今いる場所）を示す \".\" を指定\n",
        "\n",
        "# train: True を指定した場合は学習データ、False を指定した場合はテストデータをダウンロードします。\n",
        "\n",
        "# transform: 画像に対して行う前処理をdata_transform = transforms.ToTensor()指定します。今回は、transforms.ToTensor のみを指定しました。 この Transformer は、画素値を範囲が [0, 255] の uint8 型から範囲が [0, 1] の float32 型にし、Pytorch で扱う Tensor に変換を行います\n",
        "# 最低限、画像をTensorに変換する必要があるので transform=transforms.ToTensor() を指定\n",
        "\n",
        "# download: True を指定した場合は、データセットがローカルにない場合は、ネットからダウンロードします。\n",
        "# データがなければダウンロードしてほしいので download=True\n",
        "\n",
        "# これでtrain_data_loaderは、訓練ループで使うと「シャッフルされた64枚ずつの画像とラベルのバッチ」を次々と提供してくれる、便利な「供給機」になります。\n",
        "# Dataset（データのリスト）から、モデルが学習しやすいようにデータをまとめて取り出すための機能です。\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    # データを*4枚ずつの「束」（バッチ）**にして取り出すよう設\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# テストデータを読み込む DataLoader を作成する。\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root = \"datasets\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_dataset)\n",
        "\n",
        "'''\n",
        "FashionMNIST クラスの以下の属性で、クラス一覧や各画像、ラベルを取得できます。\n",
        "\n",
        "FashionMNIST.classes (list): クラス名一覧\n",
        "FashionMNIST.class_to_idx (dict): クラス名、値がクラス ID の辞書\n",
        "FashionMNIST.data (Tensor): 画像一覧\n",
        "FashionMNIST.targets (Tensor): ラベル一覧\n",
        "\n",
        "'''\n",
        "\n",
        "# 各クラスのラベルを持つサンプルを1つずつ取得する。\n",
        "class_ids, sample_indices = np.unique(train_dataset.targets, return_index=True);\n",
        "# train_dataset.targets: 学習データセット内の全画像のラベル（0: T-shirt, 1: Trouser, ... 9: Ankle boot）が格納されているリスト\n",
        "\n",
        "# np.unique(...): NumPyの関数で、重複する値を取り除いて、ユニークな値だけを返します。\n",
        "# この場合、[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] という配列が class_ids に入ります。\n",
        "\n",
        "# return_index=True: このオプションを付けると、ユニークな値が**初めて出現した位置（インデックス）**も一緒に返してくれます。\n",
        "# 例えば、「ラベル0が最初に出てきたのはデータセットの何番目か」「ラベル1が最初に出てきたのは何番目か」…という情報が sample_indices に入ります。\n",
        "# これにより、各クラスの代表サンプルを1つずつ効率的に見つけることができます。\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "# これからグラフや画像を描画するための「キャンバス」や「台紙」のようなものを作成します。figsize=(10, 4) は、キャンバスのサイズを横10インチ、縦4インチに設定しています。\n",
        "\n",
        "fig.suptitle(\"Example of every class in the Fashion-MNIST dataset だよ\",fontsize=\"x-large\")\n",
        "# 作成したキャンバス全体に大きなタイトルを設定します。\n",
        "# fontsize=\"x-large\"で文字サイズを少し大きめにしています\n",
        "\n",
        "for i in class_ids:\n",
        "  img = train_dataset.data[sample_indices[i]]\n",
        "  class_name = train_dataset.classes[i]\n",
        "\n",
        "# キャンバスを2行5列のグリッドに分割し、(i+1)番目の領域を今回の描画エリア(ax)として設定する。\n",
        "  ax = fig.add_subplot(2, 5, i + 1)\n",
        "\n",
        "# 描画エリアに、クラスIDとクラス名をタイトルとして設定する。(例: \"0: T-shirt/top\")\n",
        "  ax.set_title(f\"{i}: {class_name}\")\n",
        "\n",
        "# 画像の枠線や目盛りを非表示にする。\n",
        "  ax.set_axis_off()\n",
        "\n",
        "# 取得した画像(img)をグレースケール(cmap=\"gray\")で描画エリアに表示する。\n",
        "  ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "# これまで設定した内容をすべて画面に表示する。\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- ここからが追加するプレビューコード ---\n",
        "\n",
        "print(\"--- Data Augmentation プレビュー ---\")\n",
        "\n",
        "# Augmentation（データ拡張）の変換処理だけを定義\n",
        "# 注意: ToTensor() や Normalize() は、ここでは含めない\n",
        "#      (画像として表示しにくくなるため)\n",
        "augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    # transforms.RandomRotation(10)\n",
        "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.RandomAffine(\n",
        "        degrees=10,       # rotation_range=10 と同じ\n",
        "        scale=(0.9, 1.1)  # zoom_range=0.1 (90%~110%) と同じ\n",
        "    )\n",
        "])\n",
        "\n",
        "# 元となる画像データを1つ取得\n",
        "# train_dataset.data[0] は (28, 28) のTensor\n",
        "# 変換処理(transform)はPIL Imageを期待するため、ToPILImage()で変換\n",
        "original_image_tensor = train_dataset.data[0]\n",
        "original_pil_image = transforms.ToPILImage()(original_image_tensor)\n",
        "\n",
        "# プレビューの数\n",
        "num_previews = 5\n",
        "\n",
        "# 描画用のキャンバスを作成 (元画像 + プレビュー5枚 = 合計6枚)\n",
        "fig = plt.figure(figsize=(12, 3))\n",
        "fig.suptitle(f\"Data Augmentation ({augmenttationType}) のプレビュー\", fontsize=14)\n",
        "# fig.suptitle(\"Data Augmentationなしのプレビュー\", fontsize=14)\n",
        "\n",
        "# 1. 元画像をプロット\n",
        "ax = fig.add_subplot(1, num_previews + 1, 1)\n",
        "ax.set_title(\"Original\")\n",
        "ax.imshow(original_pil_image, cmap=\"gray\")\n",
        "ax.set_axis_off()\n",
        "\n",
        "# 2. Data Augmentationを適用した画像をプロット\n",
        "for i in range(num_previews):\n",
        "    # 同じ元画像に、毎回ランダムな変換を適用\n",
        "    transformed_image = augmentation_transform(original_pil_image)\n",
        "\n",
        "    ax = fig.add_subplot(1, num_previews + 1, i + 2)\n",
        "    ax.set_title(f\"Preview {i + 1}\")\n",
        "    ax.imshow(transformed_image, cmap=\"gray\")\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# グラフを表示\n",
        "plt.show()\n",
        "\n",
        "# --- ここまでが追加するプレビューコード ---\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」\n",
        "\n",
        "CNNモデルとは、「畳み込みニューラルネットワーク（Convolutional Neural Network）」の略で、主に画像認識の分野で圧倒的な成果を上げているディープラーニングモデルの一種です 🧠。\n",
        "ひと言でいうと、「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」です。\n",
        "\n",
        "CNNは何がすごいのか？\n",
        "従来のAIにとって、画像（例: 猫の写真）を認識するのは非常に困難でした。AIにとって画像は「ただの数字（ピクセル）の集まり」であり、猫が画像の右にいても、左にいても、大きくても、小さくても「同じ猫」だと理解するのが難しかったのです。\n",
        "\n",
        "CNNは、この問題を「畳み込み層」と「プーリング層」という特別な仕組みで解決しました。\n",
        "\n",
        "畳み込み層 (Convolutional Layer) 🔍\n",
        "\n",
        "「カーネル（フィルター）」と呼ばれる小さな\"虫眼鏡\"を使って、画像全体をスキャンします。\n",
        "この\"虫眼鏡\"は、「縦線」「横線」「特定の模様」など、画像の中の単純な特徴を探します。\n",
        "層が深くなるにつれて、単純な特徴（線）が組み合わさり、「目」や「耳」といった複雑な部品を検出できるようになります。\n",
        "\n",
        "プーリング層 (Pooling Layer) 圧縮\n",
        "\n",
        "畳み込み層が見つけた特徴マップを「圧縮」します。\n",
        "重要な特徴だけを残し、画像のサイズを小さくすることで、計算を効率化します。\n",
        "また、画像の中で特徴の位置が少しズレても、同じ特徴として認識できるようにする役割もあります（これがCNNの強力な点です）。\n",
        "\n",
        "CNNモデルとは、「畳み込みニューラルネットワーク（Convolutional Neural Network）」の略で、主に画像認識の分野で圧倒的な成果を上げているディープラーニングモデルの一種です 🧠。\n",
        "\n",
        "ひと言でいうと、「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」です。\n",
        "\n",
        "CNNは何がすごいのか？\n",
        "従来のAIにとって、画像（例: 猫の写真）を認識するのは非常に困難でした。AIにとって画像は「ただの数字（ピクセル）の集まり」であり、猫が画像の右にいても、左にいても、大きくても、小さくても「同じ猫」だと理解するのが難しかったのです。\n",
        "\n",
        "CNNは、この問題を「畳み込み層」と「プーリング層」という特別な仕組みで解決しました。\n",
        "\n",
        "CNNの主な仕組み\n",
        "CNNは、大きく分けて2つの部分で構成されています。\n",
        "\n",
        "1. 特徴抽出部（画像から特徴を見つける）\n",
        "この部分で、「畳み込み層」と「プーリング層」が活躍します。\n",
        "\n",
        "畳み込み層 (Convolutional Layer) 🔍\n",
        "\n",
        "「カーネル（フィルター）」と呼ばれる小さな\"虫眼鏡\"を使って、画像全体をスキャンします。\n",
        "この\"虫眼鏡\"は、「縦線」「横線」「特定の模様」など、画像の中の単純な特徴を探します。\n",
        "層が深くなるにつれて、単純な特徴（線）が組み合わさり、「目」や「耳」といった複雑な部品を検出できるようになります。\n",
        "\n",
        "プーリング層 (Pooling Layer) 圧縮\n",
        "\n",
        "畳み込み層が見つけた特徴マップを「圧縮」します。\n",
        "重要な特徴だけを残し、画像のサイズを小さくすることで、計算を効率化します。\n",
        "また、画像の中で特徴の位置が少しズレても、同じ特徴として認識できるようにする役割もあります（これがCNNの強力な点です）。\n",
        "\n",
        "\n",
        "2. 分類部（特徴を元に判断する）\n",
        "全結合層 (Fully Connected Layer) 🧠\n",
        "特徴抽出部が見つけ出した「目」「耳」「ひげ」といった様々な特徴の情報をすべて受け取ります。\n",
        "これらの特徴を総合的に判断し、「これだけ\"猫\"の特徴が揃っているから、この画像は99%の確率で\"猫\"だろう」と**最終的な分類（予測）**を行います。\n",
        "'''\n",
        "\n",
        "# CNN モデルを作成する\n",
        "\n",
        "# PyTorchのすべてのニューラルネットワークモデルの「親」である nn.Module を継承（引き継いで）、\n",
        "# Net という名前の新しいクラス（設計図）を作成します。\n",
        "class Net(nn.Module):\n",
        "\n",
        "  #コンストラクタと呼ばれるメソッドです。\n",
        "  # model = Net() のように、このクラス（設計図）から実際のモデル（モノ）を作るときに一度だけ呼び出されます。\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # nn.Sequentialは、複数の層をひとまとめにする「容器」です。\n",
        "    # ここでは、画像から特徴を抽出する部分（畳み込み層とプーリング層）をまとめて self.features という名前を付けています。\n",
        "    self.features = nn.Sequential(\n",
        "        # 2D畳み込み層です。\n",
        "        # 1: 入力チャンネル数（Fashion-MNISTは白黒画像なので1）\n",
        "        # 32: 出力チャンネル数（32種類の特徴を検出する）\n",
        "        # kernel_size=3: 3x3のフィルターで画像の特徴をスキャンします。\n",
        "        # padding=1: 画像のフチに1ピクセルの余白を追加します。これにより、3x3のフィルターを使っても画像のサイズ（28x28）が変わりません。\n",
        "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "\n",
        "        #**活性化関数（ReLU）**です。\n",
        "        # Conv2dで計算した結果のうち、マイナスになった値を0に置き換えます。これにより、ネットワークがより複雑なパターンを学習できるようになります。\n",
        "        nn.ReLU(),\n",
        "\n",
        "        #マックスプーリング層です。\n",
        "        # kernel_size=2: 2x2の領域を見て、その中で一番大きい値だけを残します。\n",
        "        # 役割: 画像のサイズを半分に圧縮します（例: 28x28 → 14x14）。これにより、重要な特徴だけを残し、計算量を減らします。\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        # 2層目の2D畳み込み層です。\n",
        "        # 32: 1層目の出力チャンネル数（32）と合わせる必要があります。\n",
        "        # 64: さらに多くの（64種類の）特徴を検出します。\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "\n",
        "        # 再びプーリングを行い、画像サイズをさらに半分にします（例: 14x14 → 7x7）。\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # 特徴抽出が終わった後の、最終的な「分類」を行う部分（全結合層）をまとめて self.classifier という名前を付けています。\n",
        "    self.classifier = nn.Sequential(\n",
        "\n",
        "        # ドロップアウト層です。\n",
        "        # 役割: 過学習（学習データにだけ詳しくなりすぎること）を防ぐためのテクニックです。\n",
        "        # 学習中、ランダムに一部のニューロンを「一時的に無効化」し、モデルが特定のパターンに依存しすぎるのを防ぎます。\n",
        "        nn.Dropout(),\n",
        "\n",
        "        # **全結合層（または線形層）**です。\n",
        "        # self.features の最後で、画像は 64 チャンネル、7x7 のサイズになっています。\n",
        "        # 全結合層に入力するには、この3Dデータ（64x7x7 = 3136）を1列のベクトルに「平坦化（flatten）」する必要があります。この 3136 が入力の数です。\n",
        "        nn.Linear(64 * 7 * 7, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "\n",
        "        # 最後の全結合層です。\n",
        "        # 128: 前の層の出力（128）と合わせます。\n",
        "        # 10: 最終的な出力の数。Fashion-MNISTは10クラス（Tシャツ、ズボン…など）に分類するため、必ず10になります。\n",
        "        nn.Linear(128,10),\n",
        "\n",
        "        # 最後の層の出力を「対数確率」に変換します。\n",
        "        # 10個の出力（スコア）を、合計が1になる確率に変換（Softmax）し、さらにその対数（Log）を取ります。これは、後で使う損失関数（NLLLoss）とセットで使われることが多いです。\n",
        "        nn.LogSoftmax(dim=1),\n",
        "    )\n",
        "\n",
        "  # **フォワード（順伝播）**メソッドです。\n",
        "  # __init__で定義した部品（層）を使って、データが実際に流れる順序を定義します。\n",
        "  # xは入力データ（画像のバッチ）です。\n",
        "  def forward(self,x):\n",
        "\n",
        "    # 入力データxを、self.features（Conv, ReLU, Pool, Conv, ReLU, Pool）にまとめて通します。\n",
        "    # データの形状変化: [バッチサイズ, 1, 28, 28] → [バッチサイズ, 64, 7, 7]\n",
        "    x = self.features(x)\n",
        "\n",
        "    # データを「平坦化」します。\n",
        "    # self.classifier（全結合層）に入れるために、[バッチサイズ, 64, 7, 7]のデータを[バッチサイズ, 3136]の形状に変換します。\n",
        "    # start_dim=1 は「0番目の次元（バッチサイズ）はそのまま残し、1番目以降の次元（64, 7, 7）をすべて平坦化する」という意味です。\n",
        "    x = torch.flatten(x,1)\n",
        "\n",
        "    # 平坦化されたデータxを、self.classifier（Dropout, Linear, ReLU, Dropout, Linear, LogSoftmax）にまとめて通します。\n",
        "    # データの形状変化: [バッチサイズ, 3136] → [バッチサイズ, 10]\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    # 最終的な計算結果（10クラス分の対数確率）を返します。\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 損失関数を作成する。\n",
        "# 「モデルの予測と、実際の正解との『ズレ』を計算する関数」のことです。\n",
        "nll_loss = nn.NLLLoss(reduction='sum')\n",
        "\n",
        "\n",
        "\n",
        "# 計算を実行するデバイスを選択する。\n",
        "# CUDA が利用可能な場合は、GPU、そうでない場合は CPU を選択する\n",
        "# pythonの三項演算子 A if 条件 else B\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルを計算を実行するデバイスに転送する。\n",
        "model = Net().to(device)\n",
        "\n",
        "\n",
        "# 最適化手法を選択\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "'''\n",
        "モデルの**重み（Weight）**とは、モデルが予測を行うために使う、無数の「**調整ノブ**」のようなものです。\n",
        "\n",
        "そして「**重みパラメータの更新**」とは、モデルの予測が正解に近づくように、この**「調整ノブ」を少しずつ回して調整していく作業**のことを指します。\n",
        "\n",
        "これは、機械学習モデルの「**学習**」そのものを意味する、最も重要なプロセスです。\n",
        "\n",
        "---\n",
        "\n",
        "### なぜ更新が必要なの？\n",
        "\n",
        "* **最初はデタラメ**: 新しく作ったモデルの「重み（調整ノブ）」は、最初はランダムな値（デタラメな設定）になっています。そのため、当然ながら予測もデタラメです。\n",
        "* **正解に近づけたい**: このデタラメな予測と、実際の「正解」を比べます。その「**ズレ（＝損失）**」を計算します。\n",
        "* **修正する**: その「ズレ」をできるだけ小さくするために、「こっちのノブは少し右に、あっちのノブは少し左に回そう」と、すべてのノブの設定を微調整します。これが「更新」です。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ラジオのチューニングに例えると 📻\n",
        "\n",
        "「重みの更新」は、古いラジオで放送局の電波（正解）に合わせる作業によく似ています。\n",
        "\n",
        "1.  **初期状態 (重みがランダム)**:\n",
        "    ラジオのチューニングノブ（＝重み）が適当な位置にあるため、ザーザーというノイズ（＝デタラメな予測）しか聞こえません。\n",
        "\n",
        "2.  **損失の計算 (ズレの確認)**:\n",
        "    「もっとクリアな音が聞きたい（正解）」と「今のノイズ（予測）」の**差**を感じ取ります。これが「損失」です。\n",
        "\n",
        "3.  **勾配の計算 (どっちに回すべきか？)**:\n",
        "    ノブを**右に回したら**ノイズが少しマシになったか？ **左に回したら**マシになったか？を調べます。これが「勾配（どっちの方向に進めば良くなるか）」の計算です。\n",
        "    (PyTorchでは `loss.backward()` がこれを自動でやってくれます)\n",
        "\n",
        "4.  **重みの更新 (ノブを回す)**:\n",
        "    「右に回した方が良さそうだ」と分かったら、ノブを**そっと右に回します**。\n",
        "    (PyTorchでは `optimizer.step()` がこの調整を実行します)\n",
        "\n",
        "この「**音を聞く → ズレを確認 → 回す方向を決める → そっと回す**」という作業を何度も何度も繰り返すことで、ノブ（重み）は最適な位置に調整され、電波（正解）をクリアに受信（予測）できるようになります。\n",
        "\n",
        "これがモデルの学習、すなわち「重みパラメータの更新」の正体です。\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 学習を行う関数を作成\n",
        "# model（ニューラルネットワーク）、device（GPU/CPU）、data_loader（データ供給機）、optim（最適化アルゴリズム）の4つを引数として受け取ります。\n",
        "def train(model, device, data_loader, optim):\n",
        "  \"\"\"\n",
        "    1エポック分の学習を行う。\n",
        "        :param model: モデル\n",
        "        :param device: デバイス\n",
        "        :param data_loader: Data Loader\n",
        "        :param optim: Optimizer\n",
        "  \"\"\"\n",
        "  # modelを学習モードに設定する。\n",
        "  model.train()\n",
        "\n",
        "  # このエポック（データセット1周）で発生した損失（ズレ）の合計値を記録するための変数\n",
        "  total_loss = 0\n",
        "\n",
        "  # このエポックで正解した予測の合計数を記録するための変数\n",
        "  total_correct = 0\n",
        "\n",
        "  # ★ 変更点: エポック全体の予測と正解ラベルを保存するリスト\n",
        "  all_targets = []\n",
        "  all_preds = []\n",
        "\n",
        "  # データ及びラベルを計算を実行するデバイスに転送する。\n",
        "  # data_loader（データ供給機）から、データをバッチ（束）単位で取り出すループを開始します。\n",
        "  # data には画像データ（例: 64枚分）が、target にはそれに対応する正解ラベル（例: 64個分）が入ります。\n",
        "  for data, target in data_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # 順伝搬を行う\n",
        "    # モデルに画像データ data を入力し、予測結果 output を計算させます。（forwardメソッドが実行されます）\n",
        "    output = model(data)\n",
        "\n",
        "    # 損失関数の値を計算する\n",
        "    # モデルの予測 output と、正解の target を nll_loss（損失関数）に渡し、どれだけ「ズレ」ているかを計算します。この loss の値が小さいほど、予測が正しかったことを意味します。\n",
        "    loss = nll_loss(output, target)\n",
        "    # float(loss)（または .item()）は、PyTorchのTensorからPythonの数値だけを取り出すおまじないで、メモリ効率のために重要です。\n",
        "    total_loss += float(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 逆伝搬を行う\n",
        "    # **勾配の初期化（リセット）**です。PyTorchは勾配を自動で「加算」してしまうため、このバッチの計算を始める前に、必ず前のバッチで計算した勾配をリセット（0に）します。これを忘れると学習が正しく進みません。\n",
        "    optim.zero_grad()\n",
        "    # **逆伝搬（Backward Pass）**です。\n",
        "    # 計算された loss（ズレ）を最小化するために、モデル内の全パラメータ（重み）をそれぞれどちらの方向にどれだけ調整すべきか（＝勾配）を自動で計算します。\n",
        "    loss.backward()\n",
        "\n",
        "    # パラメータを更新する\n",
        "    # optim（最適化アルゴリズム）が、loss.backward()で計算された勾配を元に、モデルの全パラメータを**実際に更新（調整）**します。\n",
        "    # これが「学習」の瞬間です。\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 確率の最も高いクラスを予測ラベルとする。\n",
        "    # モデルの output（各クラスの確率）から、最も確率が高いクラスのインデックスを予測結果 pred_target として取り出します。\n",
        "    pred_target = output.argmax(dim=1)\n",
        "\n",
        "    # 正答数を計算する\n",
        "    total_correct += int((pred_target == target).sum())\n",
        "\n",
        "    # ★ 変更点: 予測と正解ラベルをリストに溜め込む (計算用にCPUに戻す)\n",
        "    all_targets.extend(target.cpu().numpy())\n",
        "    all_preds.extend(pred_target.cpu().numpy())\n",
        "\n",
        "\n",
        "  # 損失関数の値の平均及び精度を計算する。\n",
        "  # 予測 pred_target と正解 target を比較し、True（正解）の数を合計（.sum()）して total_correct に加算します。\n",
        "  avg_loss = total_loss / len(data_loader.dataset)\n",
        "  # avg_loss = total_loss / len(data_loader.dataset)\n",
        "  accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "  # ★ 変更点: エポック全体の予測と正解を使ってメトリクスを計算\n",
        "    # average='macro': 全クラスの平均を計算 (Fahion-MNISTは10クラスあるため)\n",
        "    # zero_division=0: 警告（0除算）を非表示にする\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "      all_targets, all_preds, average='macro', zero_division=0\n",
        "  )\n",
        "\n",
        "  return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ここまで地道にやったけど、いったん完成を目指してコピペ\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 評価を行う関数を作成する\n",
        "\n",
        "def test(model, device, data_loader):\n",
        "    \"\"\"\n",
        "    テストデータに対する損失の平均及び精度を計算する。\n",
        "        :param model: モデル\n",
        "        :param device: デバイス\n",
        "        :param data_loader: Data Loader\n",
        "    \"\"\"\n",
        "    # モデルをテストモードに設定する。\n",
        "    model.eval()\n",
        "\n",
        "# ★ 変更点: エポック全体の予測と正解ラベルを保存するリスト\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        for data, target in data_loader:\n",
        "            # データ及びラベルを計算を実行するデバイスに転送する。\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # 順伝搬する。\n",
        "            output = model(data)\n",
        "\n",
        "            # 損失を計算する。\n",
        "            loss = nll_loss(output, target)\n",
        "            total_loss += float(loss)\n",
        "\n",
        "            # 確率の最も高いクラスを予測ラベルとする。\n",
        "            pred_target = output.argmax(dim=1)\n",
        "\n",
        "            # 正答数を計算する。\n",
        "            total_correct += int((pred_target == target).sum())\n",
        "\n",
        "    # ★ 変更点: 予測と正解ラベルをリストに溜め込む (計算用にCPUに戻す)\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "            all_preds.extend(pred_target.cpu().numpy())\n",
        "\n",
        "    # 損失の平均及び精度を計算する。\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "    # ★ 変更点: エポック全体の予測と正解を使ってメトリクスを計算\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_targets, all_preds, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    # ★ 変更点: 4つの値を返す\n",
        "    return avg_loss, accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "# 学習する\n",
        "n_epochs = 100\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "print('csv形式で表示')\n",
        "print(f\"epoch,{augmenttationType} [train] loss,{augmenttationType} [train] accuracy,{augmenttationType} [train] precision,{augmenttationType} [train] recall,{augmenttationType} [train] f1,{augmenttationType} [test] loss,{augmenttationType} [test] accuracy,{augmenttationType} [test] precision,{augmenttationType} [test] recall,{augmenttationType} [test] f1\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # ★ 変更点: 戻り値が5つになった\n",
        "    train_loss, train_accuracy, train_precision, train_recall, train_f1 = train(\n",
        "        model, device, train_data_loader, optim\n",
        "    )\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_accuracy\"].append(train_accuracy)\n",
        "    history[\"train_precision\"].append(train_precision)\n",
        "    history[\"train_recall\"].append(train_recall)\n",
        "    history[\"train_f1\"].append(train_f1)\n",
        "\n",
        "    # ★ 変更点: 戻り値が5つになった\n",
        "    test_loss, test_accuracy, test_precision, test_recall, test_f1 = test(\n",
        "        model, device, test_data_loader\n",
        "    )\n",
        "    history[\"test_loss\"].append(test_loss)\n",
        "    history[\"test_accuracy\"].append(test_accuracy)\n",
        "    history[\"test_precision\"].append(test_precision)\n",
        "    history[\"test_recall\"].append(test_recall)\n",
        "    history[\"test_f1\"].append(test_f1)\n",
        "\n",
        "\n",
        "\n",
        "    # ★ 変更点: print文を更新\n",
        "    print(\n",
        "        f\"epoch {epoch + 1},\"\n",
        "        f\"{train_loss:.6f},{train_accuracy:.4f},\"\n",
        "        f\"{train_precision:.4f},{train_recall:.4f},{train_f1:.4f},\"\n",
        "        f\"{test_loss:.6f},{test_accuracy:.4f},\"\n",
        "        f\"{test_precision:.4f},{test_recall:.4f},{test_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# 損失関数の値の推移をグラフ化する。\n",
        "\n",
        "epochs = np.arange(1, n_epochs + 1)\n",
        "\n",
        "# ★ 変更点: グラフを3つ (1行3列) にする\n",
        "fig, [ax1, ax2, ax3] = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# 損失の推移\n",
        "ax1.set_title(\"Loss\")\n",
        "ax1.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
        "ax1.plot(epochs, history[\"test_loss\"], label=\"test\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.legend()\n",
        "\n",
        "# 精度の推移\n",
        "ax2.set_title(\"Accuracy\")\n",
        "ax2.plot(epochs, history[\"train_accuracy\"], label=\"train\")\n",
        "ax2.plot(epochs, history[\"test_accuracy\"], label=\"test\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.legend()\n",
        "\n",
        "# ★ 変更点: F1, Precision, Recall の推移\n",
        "ax3.set_title(\"F1 / Precision / Recall\")\n",
        "ax3.plot(epochs, history[\"test_f1\"], label=\"test F1-Score\")\n",
        "ax3.plot(epochs, history[\"test_precision\"], label=\"test Precision\")\n",
        "ax3.plot(epochs, history[\"test_recall\"], label=\"test Recall\")\n",
        "ax3.set_xlabel(\"Epoch\")\n",
        "ax3.legend()\n",
        "\n",
        "# グラフをぴったり合わせる\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYMXfuzXJ3Ke"
      },
      "source": [
        "# Python標準のライブラリ。存在しないキーにアクセスした際に自動で初期値を作成する辞書(dict)機能を提供\n",
        "from collections import defaultdict\n",
        "\n",
        "# グラフ描画ライブラリ。学習の進捗や結果を可視化するために使う\n",
        "# as pltと書くことで、今後はpltという短い名前でこのライブラリの機能（例: plt.plot()）を呼び出せるように\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 数値計算ライブラリ。多次元配列(行列)を効率的に扱うために使う\n",
        "import numpy as np\n",
        "\n",
        "# PyTorchの本体。Tensorの定義や基本的な計算機能を提供\n",
        "import torch\n",
        "\n",
        "# ニューラルネットワークの層(Linear, Conv2dなど)や活性化関数、損失関数などが含まれるモジュール\n",
        "# nnの中には、全結合層 (nn.Linear) や畳み込み層 (nn.Conv2d)、活性化関数 (nn.ReLU) など、ネットワークの「層」を定義するためのクラスが多数含まれています。\n",
        "import torch.nn as nn\n",
        "\n",
        "# 画像認識用のデータセット(Fashion-MNISTなど)を簡単に利用するためのモジュール\n",
        "# torchvisionは、画像認識の分野でPyTorchを助けるためのライブラリです。\n",
        "# その中のdatasetsモジュールは、Fashion-MNISTやCIFAR10といった、\n",
        "#  機械学習の練習や研究でよく使われるデータセットを簡単にダウンロードして利用する機能を提供します。\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# 画像データに対する前処理(リサイズ、Tensorへの変換、Data Augmentationなど)を行う機能のモジュール\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "#Fashion-MNISTを読み込む\n",
        "\n",
        "# 名前を簡単にしておく\n",
        "# 画像データ（PILイメージやNumPy配列）を、PyTorchが計算で使えるTensor（テンソル）形式に変換します。\n",
        "data_transform = transforms.ToTensor()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ここに課題を作成する！！！\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# 学習データを読み込む DataLoader を作成する。\n",
        "\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root = \".\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = train_transform\n",
        "    )\n",
        "# root: データセットを保存するディレクトリのパスを指定します。\n",
        "# データを保存する場所として、カレントディレクトリ（今いる場所）を示す \".\" を指定\n",
        "\n",
        "# train: True を指定した場合は学習データ、False を指定した場合はテストデータをダウンロードします。\n",
        "\n",
        "# transform: 画像に対して行う前処理をdata_transform = transforms.ToTensor()指定します。今回は、transforms.ToTensor のみを指定しました。 この Transformer は、画素値を範囲が [0, 255] の uint8 型から範囲が [0, 1] の float32 型にし、Pytorch で扱う Tensor に変換を行います\n",
        "# 最低限、画像をTensorに変換する必要があるので transform=transforms.ToTensor() を指定\n",
        "\n",
        "# download: True を指定した場合は、データセットがローカルにない場合は、ネットからダウンロードします。\n",
        "# データがなければダウンロードしてほしいので download=True\n",
        "\n",
        "# これでtrain_data_loaderは、訓練ループで使うと「シャッフルされた64枚ずつの画像とラベルのバッチ」を次々と提供してくれる、便利な「供給機」になります。\n",
        "# Dataset（データのリスト）から、モデルが学習しやすいようにデータをまとめて取り出すための機能です。\n",
        "train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    # データを*4枚ずつの「束」（バッチ）**にして取り出すよう設\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# テストデータを読み込む DataLoader を作成する。\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root = \"datasets\",\n",
        "    train = False,\n",
        "    transform = test_transform,\n",
        "    download = True\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(train_dataset)\n",
        "\n",
        "'''\n",
        "FashionMNIST クラスの以下の属性で、クラス一覧や各画像、ラベルを取得できます。\n",
        "\n",
        "FashionMNIST.classes (list): クラス名一覧\n",
        "FashionMNIST.class_to_idx (dict): クラス名、値がクラス ID の辞書\n",
        "FashionMNIST.data (Tensor): 画像一覧\n",
        "FashionMNIST.targets (Tensor): ラベル一覧\n",
        "\n",
        "'''\n",
        "\n",
        "# 各クラスのラベルを持つサンプルを1つずつ取得する。\n",
        "class_ids, sample_indices = np.unique(train_dataset.targets, return_index=True);\n",
        "# train_dataset.targets: 学習データセット内の全画像のラベル（0: T-shirt, 1: Trouser, ... 9: Ankle boot）が格納されているリスト\n",
        "\n",
        "# np.unique(...): NumPyの関数で、重複する値を取り除いて、ユニークな値だけを返します。\n",
        "# この場合、[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] という配列が class_ids に入ります。\n",
        "\n",
        "# return_index=True: このオプションを付けると、ユニークな値が**初めて出現した位置（インデックス）**も一緒に返してくれます。\n",
        "# 例えば、「ラベル0が最初に出てきたのはデータセットの何番目か」「ラベル1が最初に出てきたのは何番目か」…という情報が sample_indices に入ります。\n",
        "# これにより、各クラスの代表サンプルを1つずつ効率的に見つけることができます。\n",
        "\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "# これからグラフや画像を描画するための「キャンバス」や「台紙」のようなものを作成します。figsize=(10, 4) は、キャンバスのサイズを横10インチ、縦4インチに設定しています。\n",
        "\n",
        "fig.suptitle(\"Example of every class in the Fashion-MNIST dataset だよ\",fontsize=\"x-large\")\n",
        "# 作成したキャンバス全体に大きなタイトルを設定します。\n",
        "# fontsize=\"x-large\"で文字サイズを少し大きめにしています\n",
        "\n",
        "for i in class_ids:\n",
        "  img = train_dataset.data[sample_indices[i]]\n",
        "  class_name = train_dataset.classes[i]\n",
        "\n",
        "# キャンバスを2行5列のグリッドに分割し、(i+1)番目の領域を今回の描画エリア(ax)として設定する。\n",
        "  ax = fig.add_subplot(2, 5, i + 1)\n",
        "\n",
        "# 描画エリアに、クラスIDとクラス名をタイトルとして設定する。(例: \"0: T-shirt/top\")\n",
        "  ax.set_title(f\"{i}: {class_name}\")\n",
        "\n",
        "# 画像の枠線や目盛りを非表示にする。\n",
        "  ax.set_axis_off()\n",
        "\n",
        "# 取得した画像(img)をグレースケール(cmap=\"gray\")で描画エリアに表示する。\n",
        "  ax.imshow(img, cmap=\"gray\")\n",
        "\n",
        "# これまで設定した内容をすべて画面に表示する。\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」\n",
        "\n",
        "CNNモデルとは、「畳み込みニューラルネットワーク（Convolutional Neural Network）」の略で、主に画像認識の分野で圧倒的な成果を上げているディープラーニングモデルの一種です 🧠。\n",
        "ひと言でいうと、「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」です。\n",
        "\n",
        "CNNは何がすごいのか？\n",
        "従来のAIにとって、画像（例: 猫の写真）を認識するのは非常に困難でした。AIにとって画像は「ただの数字（ピクセル）の集まり」であり、猫が画像の右にいても、左にいても、大きくても、小さくても「同じ猫」だと理解するのが難しかったのです。\n",
        "\n",
        "CNNは、この問題を「畳み込み層」と「プーリング層」という特別な仕組みで解決しました。\n",
        "\n",
        "畳み込み層 (Convolutional Layer) 🔍\n",
        "\n",
        "「カーネル（フィルター）」と呼ばれる小さな\"虫眼鏡\"を使って、画像全体をスキャンします。\n",
        "この\"虫眼鏡\"は、「縦線」「横線」「特定の模様」など、画像の中の単純な特徴を探します。\n",
        "層が深くなるにつれて、単純な特徴（線）が組み合わさり、「目」や「耳」といった複雑な部品を検出できるようになります。\n",
        "\n",
        "プーリング層 (Pooling Layer) 圧縮\n",
        "\n",
        "畳み込み層が見つけた特徴マップを「圧縮」します。\n",
        "重要な特徴だけを残し、画像のサイズを小さくすることで、計算を効率化します。\n",
        "また、画像の中で特徴の位置が少しズレても、同じ特徴として認識できるようにする役割もあります（これがCNNの強力な点です）。\n",
        "\n",
        "CNNモデルとは、「畳み込みニューラルネットワーク（Convolutional Neural Network）」の略で、主に画像認識の分野で圧倒的な成果を上げているディープラーニングモデルの一種です 🧠。\n",
        "\n",
        "ひと言でいうと、「画像の特徴を自動で見つけ出すのが得意な、人間の視覚に似た仕組みを持つネットワーク」です。\n",
        "\n",
        "CNNは何がすごいのか？\n",
        "従来のAIにとって、画像（例: 猫の写真）を認識するのは非常に困難でした。AIにとって画像は「ただの数字（ピクセル）の集まり」であり、猫が画像の右にいても、左にいても、大きくても、小さくても「同じ猫」だと理解するのが難しかったのです。\n",
        "\n",
        "CNNは、この問題を「畳み込み層」と「プーリング層」という特別な仕組みで解決しました。\n",
        "\n",
        "CNNの主な仕組み\n",
        "CNNは、大きく分けて2つの部分で構成されています。\n",
        "\n",
        "1. 特徴抽出部（画像から特徴を見つける）\n",
        "この部分で、「畳み込み層」と「プーリング層」が活躍します。\n",
        "\n",
        "畳み込み層 (Convolutional Layer) 🔍\n",
        "\n",
        "「カーネル（フィルター）」と呼ばれる小さな\"虫眼鏡\"を使って、画像全体をスキャンします。\n",
        "この\"虫眼鏡\"は、「縦線」「横線」「特定の模様」など、画像の中の単純な特徴を探します。\n",
        "層が深くなるにつれて、単純な特徴（線）が組み合わさり、「目」や「耳」といった複雑な部品を検出できるようになります。\n",
        "\n",
        "プーリング層 (Pooling Layer) 圧縮\n",
        "\n",
        "畳み込み層が見つけた特徴マップを「圧縮」します。\n",
        "重要な特徴だけを残し、画像のサイズを小さくすることで、計算を効率化します。\n",
        "また、画像の中で特徴の位置が少しズレても、同じ特徴として認識できるようにする役割もあります（これがCNNの強力な点です）。\n",
        "\n",
        "\n",
        "2. 分類部（特徴を元に判断する）\n",
        "全結合層 (Fully Connected Layer) 🧠\n",
        "特徴抽出部が見つけ出した「目」「耳」「ひげ」といった様々な特徴の情報をすべて受け取ります。\n",
        "これらの特徴を総合的に判断し、「これだけ\"猫\"の特徴が揃っているから、この画像は99%の確率で\"猫\"だろう」と**最終的な分類（予測）**を行います。\n",
        "'''\n",
        "\n",
        "# CNN モデルを作成する\n",
        "\n",
        "# PyTorchのすべてのニューラルネットワークモデルの「親」である nn.Module を継承（引き継いで）、\n",
        "# Net という名前の新しいクラス（設計図）を作成します。\n",
        "class Net(nn.Module):\n",
        "\n",
        "  #コンストラクタと呼ばれるメソッドです。\n",
        "  # model = Net() のように、このクラス（設計図）から実際のモデル（モノ）を作るときに一度だけ呼び出されます。\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    # nn.Sequentialは、複数の層をひとまとめにする「容器」です。\n",
        "    # ここでは、画像から特徴を抽出する部分（畳み込み層とプーリング層）をまとめて self.features という名前を付けています。\n",
        "    self.features = nn.Sequential(\n",
        "        # 2D畳み込み層です。\n",
        "        # 1: 入力チャンネル数（Fashion-MNISTは白黒画像なので1）\n",
        "        # 32: 出力チャンネル数（32種類の特徴を検出する）\n",
        "        # kernel_size=3: 3x3のフィルターで画像の特徴をスキャンします。\n",
        "        # padding=1: 画像のフチに1ピクセルの余白を追加します。これにより、3x3のフィルターを使っても画像のサイズ（28x28）が変わりません。\n",
        "        nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "\n",
        "        #**活性化関数（ReLU）**です。\n",
        "        # Conv2dで計算した結果のうち、マイナスになった値を0に置き換えます。これにより、ネットワークがより複雑なパターンを学習できるようになります。\n",
        "        nn.ReLU(),\n",
        "\n",
        "        #マックスプーリング層です。\n",
        "        # kernel_size=2: 2x2の領域を見て、その中で一番大きい値だけを残します。\n",
        "        # 役割: 画像のサイズを半分に圧縮します（例: 28x28 → 14x14）。これにより、重要な特徴だけを残し、計算量を減らします。\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "        # 2層目の2D畳み込み層です。\n",
        "        # 32: 1層目の出力チャンネル数（32）と合わせる必要があります。\n",
        "        # 64: さらに多くの（64種類の）特徴を検出します。\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "\n",
        "        # 再びプーリングを行い、画像サイズをさらに半分にします（例: 14x14 → 7x7）。\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    # 特徴抽出が終わった後の、最終的な「分類」を行う部分（全結合層）をまとめて self.classifier という名前を付けています。\n",
        "    self.classifier = nn.Sequential(\n",
        "\n",
        "        # ドロップアウト層です。\n",
        "        # 役割: 過学習（学習データにだけ詳しくなりすぎること）を防ぐためのテクニックです。\n",
        "        # 学習中、ランダムに一部のニューロンを「一時的に無効化」し、モデルが特定のパターンに依存しすぎるのを防ぎます。\n",
        "        nn.Dropout(),\n",
        "\n",
        "        # **全結合層（または線形層）**です。\n",
        "        # self.features の最後で、画像は 64 チャンネル、7x7 のサイズになっています。\n",
        "        # 全結合層に入力するには、この3Dデータ（64x7x7 = 3136）を1列のベクトルに「平坦化（flatten）」する必要があります。この 3136 が入力の数です。\n",
        "        nn.Linear(64 * 7 * 7, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(),\n",
        "\n",
        "        # 最後の全結合層です。\n",
        "        # 128: 前の層の出力（128）と合わせます。\n",
        "        # 10: 最終的な出力の数。Fashion-MNISTは10クラス（Tシャツ、ズボン…など）に分類するため、必ず10になります。\n",
        "        nn.Linear(128,10),\n",
        "\n",
        "        # 最後の層の出力を「対数確率」に変換します。\n",
        "        # 10個の出力（スコア）を、合計が1になる確率に変換（Softmax）し、さらにその対数（Log）を取ります。これは、後で使う損失関数（NLLLoss）とセットで使われることが多いです。\n",
        "        nn.LogSoftmax(dim=1),\n",
        "    )\n",
        "\n",
        "  # **フォワード（順伝播）**メソッドです。\n",
        "  # __init__で定義した部品（層）を使って、データが実際に流れる順序を定義します。\n",
        "  # xは入力データ（画像のバッチ）です。\n",
        "  def forward(self,x):\n",
        "\n",
        "    # 入力データxを、self.features（Conv, ReLU, Pool, Conv, ReLU, Pool）にまとめて通します。\n",
        "    # データの形状変化: [バッチサイズ, 1, 28, 28] → [バッチサイズ, 64, 7, 7]\n",
        "    x = self.features(x)\n",
        "\n",
        "    # データを「平坦化」します。\n",
        "    # self.classifier（全結合層）に入れるために、[バッチサイズ, 64, 7, 7]のデータを[バッチサイズ, 3136]の形状に変換します。\n",
        "    # start_dim=1 は「0番目の次元（バッチサイズ）はそのまま残し、1番目以降の次元（64, 7, 7）をすべて平坦化する」という意味です。\n",
        "    x = torch.flatten(x,1)\n",
        "\n",
        "    # 平坦化されたデータxを、self.classifier（Dropout, Linear, ReLU, Dropout, Linear, LogSoftmax）にまとめて通します。\n",
        "    # データの形状変化: [バッチサイズ, 3136] → [バッチサイズ, 10]\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    # 最終的な計算結果（10クラス分の対数確率）を返します。\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 損失関数を作成する。\n",
        "# 「モデルの予測と、実際の正解との『ズレ』を計算する関数」のことです。\n",
        "nll_loss = nn.NLLLoss()\n",
        "\n",
        "\n",
        "\n",
        "# 計算を実行するデバイスを選択する。\n",
        "# CUDA が利用可能な場合は、GPU、そうでない場合は CPU を選択する\n",
        "# pythonの三項演算子 A if 条件 else B\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# モデルを計算を実行するデバイスに転送する。\n",
        "model = Net().to(device)\n",
        "\n",
        "\n",
        "# 最適化手法を選択\n",
        "optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "'''\n",
        "モデルの**重み（Weight）**とは、モデルが予測を行うために使う、無数の「**調整ノブ**」のようなものです。\n",
        "\n",
        "そして「**重みパラメータの更新**」とは、モデルの予測が正解に近づくように、この**「調整ノブ」を少しずつ回して調整していく作業**のことを指します。\n",
        "\n",
        "これは、機械学習モデルの「**学習**」そのものを意味する、最も重要なプロセスです。\n",
        "\n",
        "---\n",
        "\n",
        "### なぜ更新が必要なの？\n",
        "\n",
        "* **最初はデタラメ**: 新しく作ったモデルの「重み（調整ノブ）」は、最初はランダムな値（デタラメな設定）になっています。そのため、当然ながら予測もデタラメです。\n",
        "* **正解に近づけたい**: このデタラメな予測と、実際の「正解」を比べます。その「**ズレ（＝損失）**」を計算します。\n",
        "* **修正する**: その「ズレ」をできるだけ小さくするために、「こっちのノブは少し右に、あっちのノブは少し左に回そう」と、すべてのノブの設定を微調整します。これが「更新」です。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ラジオのチューニングに例えると 📻\n",
        "\n",
        "「重みの更新」は、古いラジオで放送局の電波（正解）に合わせる作業によく似ています。\n",
        "\n",
        "1.  **初期状態 (重みがランダム)**:\n",
        "    ラジオのチューニングノブ（＝重み）が適当な位置にあるため、ザーザーというノイズ（＝デタラメな予測）しか聞こえません。\n",
        "\n",
        "2.  **損失の計算 (ズレの確認)**:\n",
        "    「もっとクリアな音が聞きたい（正解）」と「今のノイズ（予測）」の**差**を感じ取ります。これが「損失」です。\n",
        "\n",
        "3.  **勾配の計算 (どっちに回すべきか？)**:\n",
        "    ノブを**右に回したら**ノイズが少しマシになったか？ **左に回したら**マシになったか？を調べます。これが「勾配（どっちの方向に進めば良くなるか）」の計算です。\n",
        "    (PyTorchでは `loss.backward()` がこれを自動でやってくれます)\n",
        "\n",
        "4.  **重みの更新 (ノブを回す)**:\n",
        "    「右に回した方が良さそうだ」と分かったら、ノブを**そっと右に回します**。\n",
        "    (PyTorchでは `optimizer.step()` がこの調整を実行します)\n",
        "\n",
        "この「**音を聞く → ズレを確認 → 回す方向を決める → そっと回す**」という作業を何度も何度も繰り返すことで、ノブ（重み）は最適な位置に調整され、電波（正解）をクリアに受信（予測）できるようになります。\n",
        "\n",
        "これがモデルの学習、すなわち「重みパラメータの更新」の正体です。\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 学習を行う関数を作成\n",
        "# model（ニューラルネットワーク）、device（GPU/CPU）、data_loader（データ供給機）、optim（最適化アルゴリズム）の4つを引数として受け取ります。\n",
        "def train(model, device, data_loader, optim):\n",
        "  \"\"\"\n",
        "    1エポック分の学習を行う。\n",
        "        :param model: モデル\n",
        "        :param device: デバイス\n",
        "        :param data_loader: Data Loader\n",
        "        :param optim: Optimizer\n",
        "  \"\"\"\n",
        "  # modelを学習モードに設定する。\n",
        "  model.train()\n",
        "\n",
        "  # このエポック（データセット1周）で発生した損失（ズレ）の合計値を記録するための変数\n",
        "  total_loss = 0\n",
        "\n",
        "  # このエポックで正解した予測の合計数を記録するための変数\n",
        "  total_correct = 0\n",
        "\n",
        "  # データ及びラベルを計算を実行するデバイスに転送する。\n",
        "  # data_loader（データ供給機）から、データをバッチ（束）単位で取り出すループを開始します。\n",
        "  # data には画像データ（例: 64枚分）が、target にはそれに対応する正解ラベル（例: 64個分）が入ります。\n",
        "  for data, target in data_loader:\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # 順伝搬を行う\n",
        "    # モデルに画像データ data を入力し、予測結果 output を計算させます。（forwardメソッドが実行されます）\n",
        "    output = model(data)\n",
        "\n",
        "    # 損失関数の値を計算する\n",
        "    # モデルの予測 output と、正解の target を nll_loss（損失関数）に渡し、どれだけ「ズレ」ているかを計算します。この loss の値が小さいほど、予測が正しかったことを意味します。\n",
        "    loss = nll_loss(output, target)\n",
        "    # float(loss)（または .item()）は、PyTorchのTensorからPythonの数値だけを取り出すおまじないで、メモリ効率のために重要です。\n",
        "    total_loss += float(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 逆伝搬を行う\n",
        "    # **勾配の初期化（リセット）**です。PyTorchは勾配を自動で「加算」してしまうため、このバッチの計算を始める前に、必ず前のバッチで計算した勾配をリセット（0に）します。これを忘れると学習が正しく進みません。\n",
        "    optim.zero_grad()\n",
        "    # **逆伝搬（Backward Pass）**です。\n",
        "    # 計算された loss（ズレ）を最小化するために、モデル内の全パラメータ（重み）をそれぞれどちらの方向にどれだけ調整すべきか（＝勾配）を自動で計算します。\n",
        "    loss.backward()\n",
        "\n",
        "    # パラメータを更新する\n",
        "    # optim（最適化アルゴリズム）が、loss.backward()で計算された勾配を元に、モデルの全パラメータを**実際に更新（調整）**します。\n",
        "    # これが「学習」の瞬間です。\n",
        "    optim.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 確率の最も高いクラスを予測ラベルとする。\n",
        "    # モデルの output（各クラスの確率）から、最も確率が高いクラスのインデックスを予測結果 pred_target として取り出します。\n",
        "    pred_target = output.argmax(dim=1)\n",
        "\n",
        "    # 正答数を計算する\n",
        "    total_correct += int((pred_target == target).sum())\n",
        "\n",
        "  # 損失関数の値の平均及び精度を計算する。\n",
        "  # 予測 pred_target と正解 target を比較し、True（正解）の数を合計（.sum()）して total_correct に加算します。\n",
        "  avg_loss = total_loss / len(data_loader.dataset)\n",
        "  # avg_loss = total_loss / len(data_loader.dataset)\n",
        "  accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "  return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "ここまで地道にやったけど、いったん完成を目指してコピペ\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 評価を行う関数を作成する\n",
        "\n",
        "def test(model, device, data_loader):\n",
        "    \"\"\"\n",
        "    テストデータに対する損失の平均及び精度を計算する。\n",
        "        :param model: モデル\n",
        "        :param device: デバイス\n",
        "        :param data_loader: Data Loader\n",
        "    \"\"\"\n",
        "    # モデルをテストモードに設定する。\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        for data, target in data_loader:\n",
        "            # データ及びラベルを計算を実行するデバイスに転送する。\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # 順伝搬する。\n",
        "            output = model(data)\n",
        "\n",
        "            # 損失を計算する。\n",
        "            loss = nll_loss(output, target)\n",
        "            total_loss += float(loss)\n",
        "\n",
        "            # 確率の最も高いクラスを予測ラベルとする。\n",
        "            pred_target = output.argmax(dim=1)\n",
        "\n",
        "            # 正答数を計算する。\n",
        "            total_correct += int((pred_target == target).sum())\n",
        "\n",
        "    # 損失の平均及び精度を計算する。\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 学習する\n",
        "n_epochs = 50\n",
        "\n",
        "history = defaultdict(list)\n",
        "for epoch in range(n_epochs):\n",
        "    # 1エポック分、学習する。\n",
        "    train_loss, train_accuracy = train(model, device, train_data_loader, optim)\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_accuracy\"].append(train_accuracy)\n",
        "\n",
        "    # 評価する。\n",
        "    test_loss, test_accuracy = test(model, device, test_data_loader)\n",
        "    history[\"test_loss\"].append(test_loss)\n",
        "    history[\"test_accuracy\"].append(test_accuracy)\n",
        "\n",
        "    print(\n",
        "        f\"epoch {epoch + 1} \"\n",
        "        f\"[train] loss: {train_loss:.6f}, accuracy: {train_accuracy:.0%} \"\n",
        "        f\"[test] loss: {test_loss:.6f}, accuracy: {test_accuracy:.0%}\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 損失関数の値の推移をグラフ化する。\n",
        "\n",
        "epochs = np.arange(1, n_epochs + 1)\n",
        "\n",
        "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(8, 3))\n",
        "\n",
        "# 損失の推移\n",
        "ax1.set_title(\"Loss\")\n",
        "ax1.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
        "ax1.plot(epochs, history[\"test_loss\"], label=\"test\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.legend()\n",
        "\n",
        "# 精度の推移\n",
        "ax2.set_title(\"Accuracy\")\n",
        "ax2.plot(epochs, history[\"train_accuracy\"], label=\"train\")\n",
        "ax2.plot(epochs, history[\"test_accuracy\"], label=\"test\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOmWoaR33NpViHIy813zDSE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}